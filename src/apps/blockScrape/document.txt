#if 0
bool consolidate_chunks() {
    blknum_t nRecords = fileSize(new Stage) / 5 9;
    blknum_t chunkSize = opts->apps_per_chunk;
    int64_t distToHead = (int64_t(chunkSize) - int64_t(nRecords));
    if (nRecords <= chunkSize)
        Consolidation not ready...
        return

    write_chunks()
}

int main(int argc, const char* argv[]) {
    parseArguments()
    scrape_blocks();
}

func parseArguments() {
    // Command line options
    //      pin          pin chunks (and blooms) to IPFS as they are created (requires pinning service)
    //      publish      after pinning the chunk, publish it to UnchainedIndex
    //      block_cnt    maximum number of blocks to process per pass

    // Configurable items
    //      block_cnt           2000         maximum number of blocks to process (defaults to 5000)
    //      block_chan_cnt      10           number of concurrent block channels for blaze
    //      addr_chan_cnt       20           number of concurrent address channels for blaze
    //      apps_per_chunk      2000000
    //      unripe_dist         28
    //      snap_to_grid        100000
    //      first_snap          2250000
    //      allow_missing       false
    //      n_test_runs         0

    // Establish folders
    // cache/<chain>/monitors
    // unchained/<chain>
    //      finalized
    //      blooms
    //      staging
    //      unripe
    //      ripe
    //      tmp

    // Validation
    //      --pin requires an API key for pinata
    //      must be running against a tracing node
    //      must be running against an archive node
 
    // Environment
    //      don't run if acctExport is running
    //      clean away any unused data from
    //          possbile previously incomplete scrape
    //          by removing unripe data

    // Scraping
    //      if block zero index does not exist, create it
}

bool scrape_blocks(void) {

    // Using a consolidator object....
    //      carries progress, pin option, block_cnt

    // Figure out where to start this scrape...
    //      start = max(ripe, staging, finalized) + 1
    //      prog(ripe), prog(staging), prog(finalized)

    // Figure out ripeBlock (unripe_dist from head of chain)
    //      all blocks older than unripe_dist from head are ripe
    //      ripe blocks are old enough (default 28 blocks, 6 minutes)
    //          so that we don't have to consider them any longer
    //      the blaze scraper won't scrape past ripeBlock
    //      once a block is ripe (i.e., staged) we no longer scrape it
    //      until then, the block is unripe, and we will rescrape it

    // Staging (i.e., ripeBlocks) are 'ready to be consolidated' and will
    // be as soon as we see the number of appearances we are looking for
    // ripeBlock = front oh chain - unripe_dist

    // distFromHead = chainHead - blaze_start
    // blazeCount = min(distFromHead, blazeCount)

    // if start > client don't run

    // Call blaze, if blaze return false or acctExport is running, cleanup and start over next time
    if (!blaze() || isRunning("acctScrape"))
        return false;

    // move all files in the ripe folder into a single file called staging and remove
    // each ripe file. If interrupted, clean out the ripe, unripe, and staging folders
    // so we can start over next time

    // At this point, staging is a single file containing all records that have not
    // been consolidated. Fixed width, sorted.
    // Ripe is empty
    // Unripe has blocks less than six minutes old

    // Try to write chunks
    stage_ chunks()

    consolidate_chunks()
}

bool stage_ chunks(void) {
    old Stage = getLastFileInFolder(indexFolder_staging, false);
    new Stage = indexFolder_staging + padNum9(prev Block) + ".txt";
    if (old Stage == new Stage)
        LOG4(bBlue, "Consolidation not ready...", cOff);
        return

    tmp File = indexFolder + "temp.txt";
    if (old Stage != tmp _fn)
        appendFile(tmp File, old Stage)
        
    appendFile(tmp File, tmp _fn)
    lockSection();
    ::rename(tmp File.c_str(), new Stage.c_str());
    cleaup
}

bool write_chunks() {
    blknum_t nRecords = fileSize(new Stage) / 5 9;
    while ((atLeastOnce || nRecords > chunkSize) && !shouldQuit()) {
        // read lines in staging file

        // starting one line less than the chunk size
        // look for the next change in block number after the chunk size
}
#endif